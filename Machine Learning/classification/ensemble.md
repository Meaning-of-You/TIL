# Ensemble Learning(앙상블 학습)

> 앙상블 학습 정리



### 앙상블 학습

여러 개의 분류기를 생성하고 그 예측을 결합함으로써 보다 정확한 최종 예측을 도출하는 기법

- 비정형 데이터 분류는 딥러닝, 정형 데이터 분류 시에는 앙상블이 뛰어난 성능을 나타냄



### 앙상블 학습의 유형

##### 보팅(Voting)

여러 개의 분류기가 투표를 통해 최종 예측 결과를 결정하는 방식으로, 서로 다른 알고리즘을 가진 분류기를 결합하는 것

- 하드 보팅(Hard Voting): 예측한 결과값들 중 다수의 분류기가 결정한 예측값을 최종 보팅 결과값으로 선정
- 소프트 보팅(Soft Voting): 분류기들의 레이블 값 결정 확률을 모두 더하고 이를 평균해서 이들 중 확률이 가장 높른 레이블 값을 최종 보팅 결과값으로 선정

##### 배깅(Bagging)

보팅과 유사한 방식, 각각의 분류기가 모두 같은 유형의 알고리즘 기반이지만, 데이터 샘플링을 서로 다르게 가져가면서 학습을 수행해 보팅을 수행하는 것

- `부트스트래핑(Bootstrapping)`: 개별 Classifier에게 데이터를 샘플링해서 추출하는 방식
- 배깅 방식은 중첩을 허용
- 대표적인 배깅 방식이 Random Forest 알고리즘

##### 부스팅(Boosting)

여러 개의 분류기가 순차적으로 학습을 수행하되, 앞에서 학습한 분류기가 예측이 틀린 데이터에 대해서는 올바르게 예측할 수 있도록 다음 분류기에는 가중치를 부여하면서 학습과 예측을 진행하는 것

- 대표적으로 Gradient Boosting, XGBoost(eXtra Gradient Boost), LightGBM(Light Gradient Boost)가 있음

##### 스태킹(Stacking)

여러 가지 다른 모델의 예측 결괏값을 다시 학습 데이터로 만들어서 다른 모델(메타 모델)로 재학습시켜 결과를 예측하는 것
