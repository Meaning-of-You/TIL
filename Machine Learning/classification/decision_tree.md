# Decision Tree(결정 트리)

> 머신러닝 알고리즘인 Decision Tree 정리



### 결정 트리

데이터에 있는 규칙을 학습을 통해 자동으로 찾아내 트리(Tree) 기반의 분류 규칙을 만드는 것

##### 결정 트리의 구조

![img](https://blog.kakaocdn.net/dn/dy5OwG/btqDwdHofoT/NtDy9lqXkhWTRTwEz6txd0/img.png)

- `규칙 노드(Decision Node)`: 규칙 조건이 되는 것
  - 데이터 세트에 피처가 있고 이러한 피처가 결합해 규칙 조건을 만들 때마다 생성
  - 트리의 깊이(depth)가 깊어질수록 결정 트리의 예측 성능이 저하될 가능성이 높음
- `리프 노드(Leaf Node)`: 결정된 클래스 값
- 새로운 규칙 조건마다 `서브 트리(sub tree)`가 생성



##### 결정 트리의 특징

| 장점                                                         | 단점                                |
| :----------------------------------------------------------- | :---------------------------------- |
| - 쉽고 직관적임<br />- 피처의 스케일링이나 정규화 등의 사전 가공 영향도가 크지 않음 | - 과적합으로 알고리즘 성능이 떨어짐 |



##### 결정 트리 파라미터

| 파라미터 명       | 설명                                                         |
| ----------------- | ------------------------------------------------------------ |
| min_samples_split | - 노드를 분할하기 위한 최소한의 샘플 데이터 수로 과적합을 제어하는 데 사용<br />- default는 `2`이고 작게 설정할수록 분할되는 노드가 많아져서 과적합 가능성 증가 |
| min_samples_leaf  | - 말단 노드가 되기 위한 최소한의 샘플 데이터 수로 과적합을 제어하는 데 사용<br />- 비대칭적 데이터의 경우 특정 클래스의 데이터가 극도로 작을 수 있으므로 이 경우는 작게 설정 |
| max_features      | - 최적의 분할을 위해 고려할 최대 피처 개수, default는 `None`으로 데이터 세트와 모든 피처를 사용해 분할 수행<br />- `int형`: 대상 피처의 개수<br />- `float형`: 전체 피처 중 대상 피터의 퍼센트<br />- `sqrt(auto)`: 전체 피처 중 sqrt(전체 피처 개수) <br />- `log`: 전체 피처 중 log2(전체 피처 개수) |
| max_depth         | - 트리의 최대 깊이를 규정<br />- default는 `None`으로 완벽하게 클래스 결정 값이 될 때까지 깊이를 계속 키우며 분할<br />- 깊이가 깊어지면 min_samples_split 설정대로 최대 분할하여 과적합 할 수 있으므로 적절한 값으로 제어 |
| max_leaf_nodes    | - 말단 노드의 최대 개수                                      |



### 정보 이득(Information Gain) 지수와 지니 계수

정보의 균일도를 측정하는 방법 2가지

- 정보 이득: 1에서 엔트로피 지수를 뺀 값
  -  엔트로피: 주어진 데이터 집합의 혼잡도, 서로 다른 값이 섞여 있으면 엔트로피가 높고, 같은 값이 섞여 있으면 엔트로피가 낮음
  - 정보 이득이 높은 속성을 기준으로 분할
- 지니 계수: 불평등 지수를 나타낼 때 사용
  - 0이 가장 평등하고 1로 갈수록 불평등
  - 지니 계수가 낮은 속성을 기준으로 분할



### 결정 트리 과적합(Overfitting)

![2](https://img1.daumcdn.net/thumb/R720x0.q80/?scode=mtistory2&fname=http%3A%2F%2Fcfile5.uf.tistory.com%2Fimage%2F99F10B4E5B72A94E161376)

- 복잡한 모델은 학습 데이터 세트의 특성과 약간만 다른 형태의 데이터 세트를 예측하면 예측 정확도가 떨어짐
  - 테스트 데이터 세트는 학습 데이터 세트와는 다른 데이터 세트인데, 학습 데이터에만 지나치게 최적화된 분류 기준은 테스트 데이터 세트에서 정확도를 떨어뜨림

