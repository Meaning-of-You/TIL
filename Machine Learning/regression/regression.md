# Regression(회귀)

> 회귀 분석 정리



### 지도 학습

##### 분류(Classification)

- 예측값이 카테고리와 같은 이산형 클래스 값

##### 회귀(Regression)

- 예측값이 연속형 숫자 값



### 회귀 분석

데이터 값이 평균과 같은 일정한 값으로 돌아가려는 경향을 이용

##### 회귀

여러 개의 독립변수(피처)와 한 개의 종속변수(결정 값) 간의 상관관계를 모델링하는 기법을 통칭

- 회귀 유형

| 독립변수 개수      | 회귀 계수의 결합    |
| ------------------ | ------------------- |
| 1개: 단일 회귀     | 선형: 선형 회귀     |
| 여러 개: 다중 회귀 | 비선형: 비선형 회귀 |

##### 선형 회귀

실제 값과 예측값의 차이(오류의 제곱 값)를 최소화하는 직선형 회귀선을 최적화하는 방식

- 피처값과 타깃값의 분포가 정규 분포인 형태를 선호
  - 타깃값의 경우 일반적으로 로그 변환 적용

- 종류
  - 일반 선형 회귀: 예측값과 실제 값의 RSS(Residual Sum of Squares)를 최소화할 수 있도록 회귀 계수를 최적화하며, 규제(Regularization)를 적용하지 않은 모델
  - 릿지(Ridge): 선형 회귀에 L2 규제를 추가한 회귀 모델
  - 라쏘(Lasso): 선형 회귀에 L1 규제를 적용한 방식
  - 엘라스틱넷(ElasticNet): L2, L1 규제를 함께 결합한 모델
  - 로지스틱 회귀(Logistic Regression): 분류에 사용되는 선형 모델

##### 규제

- L1 규제: 예측 영향력이 작은 피처의 회귀 계수를 0으로 만들어 회귀 예측 시 피처가 선택되지 않게 하는 것
- L2 규제: 상대적으로 큰 회귀 계수 값의 예측 영향도를 감소시키기 위해서 회귀계수 값을 더 작게 만드는 규제 모델



### 단순 선형 회귀

독립변수도 하나, 종속변수도 하나인 선형 회귀

- 최적의 회귀 모델을 만든다는 것은 전체 데이터의 잔차 합이 최소가 되는 모델을 만든다는 의미
  - 잔차: 실제 값과 회귀 모델의 차이에 따른 오류 값
- 오류 합을 계산할 때는 절댓값을 취해서 더하거나(Mean Absolute Error), 오류 값의 제곱을 구해서 더하는 방식(Residual Sum of Squares) 사용

- 비용 함수: RSS는 비용, w변수(회귀 계수)로 구성되는 RSS
  - 경사하강법: 점진적으로 반복적인 계산을 통해 W 파라미터 값을 업데이트 하면서 오류 값이 최소가 되는 W 파라미터를 구함

##### 회귀 평가 지표

|           평가 지표           | scikit-learn API            | 설명                                                        |
| :---------------------------: | --------------------------- | ----------------------------------------------------------- |
|   MAE(Mean Absolute Error)    | metrics.mean_absolute_error | 실제 값과 예측값의 차이를 절댓값으로 변환해 평균한 것       |
|    MSE(Mean Squared Error)    | metrics.mean_squared_error  | 실제 값과 예측값의 차이를 제곱해 평균한 것                  |
| RMSE(Root Mean Squared Error) | 제공 X                      | MSE에 루트를 씌운 것                                        |
|              R^2              | mterics.r2_score            | 실제 값의 분산 대비 예측값의 분산 비율, 1에 가까울수록 좋음 |



### 다항 회귀

회귀가 2차, 3차 방정식과 같은 다항식으로 표현되는 것

##### 과적합 문제

다항 회귀의 차수를 높일수록 학습 데이터에만 너무 맞춘 학습이 이뤄져서 테스트 데이터 환경에서는 오히려 예측 정확도가 떨어짐

##### 편향-분산 트레이드오프

![Bias-variance-tradeoff-1.png](https://z-images.s3.amazonaws.com/thumb/f/f7/Bias-variance-tradeoff-1.png/330px-Bias-variance-tradeoff-1.png)

편향과 분산은 한 쪽이 높으면 다른 한 쪽은 낮아지는 경향이 있음

- 편향이 높으면 분산은 낮아지고(과소적합), 분산이 높으면 편향이 낮아짐(과적합)



### 규제 선형 모델

##### 릿지 회귀

L2규제를 적용한 회귀로, W의 제곱에 대해 페널티를 부여

- L2규제가 회귀 계수의 크기를 감소시킴

##### 라쏘 회귀

L1규제를 적용한 회귀로, W의 절댓값에 대해 페널티를 부여

- L1 규제는 불필요한 회귀 계수를 급격하게 감소시켜 0으로 만들고 제거
- 적절한 피터만 회귀에 포함시키는 피처 선택의 특성을 가짐

##### 엘라스틱넷 회귀

L2규제와 L1규제 결합

- 라쏘 회귀의 경우 회귀 계수의 값이 급격히 변동할 수도 있는 데, 이를 완화하기 위해 L2 규제를 추가
- 수행시간이 상대적으로 오래 걸림



### 로지스틱 회귀

선형 회귀 방식을 분류에 적용

![sig](https://mlnotebook.github.io/img/transferFunctions/sigmoid.png)

- 학습을 통해 선형 함수의 회귀 최적선을 찾는 것이 아니라 시그모이드 함수 최적선을 찾고 이 시그모이드 함수의 반환 값을 확률로 간주해 확률에 따라 분류 결정